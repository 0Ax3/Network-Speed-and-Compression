### Network-acceleration
- https://github.com/yihui-he/channel-pruning

### Network Compression
- [Learning both Weights and Connections for Efficient Neural Network.](https://arxiv.org/pdf/1506.02626.pdf)
- [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.](http://arxiv.org/abs/1510.00149)
- [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size.](http://arxiv.org/abs/1602.07360)
- [8-Bit Approximations for Parallelism in Deep Learning.](http://arxiv.org/abs/1511.04561)
- [Neural Networks with Few Multiplications.](https://arxiv.org/abs/1510.03009)
- [Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications.](http://arxiv.org/abs/1511.06530) 
- [Hardware-oriented Approximation of Convolutional Neural Networks.](https://arxiv.org/abs/1604.03168)
- [Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets.](https://arxiv.org/abs/1511.05236)
- [Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations.](http://arxiv.org/abs/1609.07061)
- [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients.](http://arxiv.org/abs/1606.06160)
- [Deep Learning with Limited Numerical Precision.](https://arxiv.org/abs/1502.02551)
- [Dynamic Network Surgery for Efficient DNNs.](http://arxiv.org/abs/1608.04493)
- [Understanding the Impact of Precision Quantization on the Accuracy and Energy of Neural Networks.](https://arxiv.org/abs/1612.03940) 
- [Variational Dropout Sparsifies Deep Neural Networks](https://arxiv.org/pdf/1701.05369.pdf) https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn
- [Soft Weight-Sharing for Neural Network Compression](https://arxiv.org/pdf/1702.04008.pdf)
- [LCNN: Lookup-based Convolutional Neural Network](https://arxiv.org/pdf/1611.06473.pdf)
- [Bayesian Compression for Deep Learning](https://arxiv.org/pdf/1705.08665.pdf)
- [ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression](https://github.com/Roll920/ThiNet)

### Network quantization
- https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/

### Binary networks
- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks.](http://arxiv.org/abs/1603.05279)
- [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1.](http://arxiv.org/abs/1602.02830)
- [BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet](https://github.com/hpi-xnor/BMXNet)

### Mobile devices
- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf)
    + https://github.com/Zehaos/MobileNet
- [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/pdf/1707.01083.pdf)
- [CondenseNet: An Efficient DenseNet using Learned Group Convolutions](https://arxiv.org/pdf/1711.09224.pdf)
    + https://github.com/ShichenLiu/CondenseNet

To look at:
~~~
https://github.com/dkozlov/awesome-knowledge-distillation
https://github.com/ljk628/ML-Systems/blob/master/dl_cnn.md
https://github.com/songhan/SqueezeNet-Deep-Compression
https://github.com/jiaxiang-wu/quantized-cnn
https://github.com/andyhahaha/Convolutional-Neural-Network-Compression-Survey
https://github.com/Zhouaojun/Efficient-Deep-Learning
~~~
